\documentclass{ximera}
\input{../preamble.tex}

\author{Paul Zachlin \and Anna Davis} \title{Orthogonal and Orthonormal Sets of Vectors} \license{CC-BY 4.0}

\begin{document}
\begin{abstract}

\end{abstract}
\maketitle
\subsection{Orthogonal and Orthonormal Sets}

In this section, we examine what it means for vectors (and sets of
vectors) to be orthogonal and orthonormal. Recall that two non-zero vectors are orthogonal if their dot product is zero.  (In $\RR^2$ and $\RR^3$ this means that two vectors are perpendicular.)  A collection of non-zero vectors in $\RR^n$ is called \dfn{orthogonal} if the vectors are pair-wise orthogonal.  The diagram below shows two orthogonal vectors in $\RR^2$ and three orthogonal vectors in $\RR^3$.
\begin{center}
\begin{tikzpicture}[scale=0.5]
\draw[line width=2pt,red,-stealth](0,0)--(1.5,0.5);  
\draw[line width=2pt,blue,-stealth](0,0)--(-1,3);
\node[] at (-1, -1)  (p2)    {Orthogonal vectors in $\RR^2$};
 \end{tikzpicture}
 \quad\quad\quad
\begin{tikzpicture}[scale=0.5]
	\draw[line width=2pt,red,-stealth](0,0,0)--(5,0,0);
    \draw[line width=2pt,blue,-stealth](0,0,0)--(0,2,0);
    \draw[line width=2pt,black,-stealth](0,0,0)--(0,0,5);
    \node[] at (3, -3,0)  (p2)    {Orthogonal vectors in $\RR^3$};
    \end{tikzpicture}
\end{center}

If every vector in an orthogonal set of vectors is also a unit vector, then we say that the given set of vectors is \dfn{orthonormal}.

\begin{center}
\begin{tikzpicture}[scale=0.25]
\draw[line width=2pt,red,-stealth](0,0)--(3,4);  
\draw[line width=2pt,blue,-stealth](0,0)--(-4,3);
\node[] at (-2, -2)  (p2)    {Orthonormal vectors in $\RR^2$};
 \end{tikzpicture}
 \quad\quad\quad
\tdplotsetmaincoords{70}{130}
\begin{tikzpicture}[scale=0.25]
	\draw[line width=2pt,red,-stealth](0,0,0)--(5,0,0);
    \draw[line width=2pt,blue,-stealth](0,0,0)--(0,5,0);
    \draw[line width=2pt,black,-stealth](0,0,0)--(0,0,5);
    \node[] at (3, -3,0)  (p2)    {Orthonormal vectors in $\RR^3$};
    \end{tikzpicture}
\end{center}

Formally, we can define orthogonal and orthonormal vectors as follows.

\begin{definition}\label{orthset}
Let $\{ \vec{x}_1, \vec{x}_2, \cdots, \vec{x}_m \}$ be a set of nonzero
vectors in $\RR^n$. Then this set is called an
\dfn{orthogonal set} if 
$\vec{x}_i \dotp \vec{x}_j = 0$ for all $i \neq j$.
Moreover, if $\norm{\vec{v}_i}=1$ for $i=1,\ldots,m$ (i.e. each vector in the set is a unit vector), we say the set of vectors is an \dfn{orthonormal set}.
\end{definition}

An orthogonal set of vectors may not be orthonormal.  To convert an orthogonal set to an orthonormal set, we simply need divide each vector by its own length.

\begin{definition}\label{normalizing}
\dfn{Normalizing} an orthogonal set is the process of turning an orthogonal set into an orthonormal set.
If $\{ \vec{v}_1, \vec{v}_2, \ldots, \vec{v}_k\}$
is an orthogonal subset of $\RR^n$,
then
\[ \left\{
\frac{1}{\norm{\vec{v}_1}}\vec{v}_1,
\frac{1}{\norm{\vec{v}_2}}\vec{v}_2, \ldots,
\frac{1}{\norm{\vec{v}_k}}\vec{v}_k \right\}
\]
is an orthonormal set.
\end{definition}

We illustrate this concept in the following example.

\begin{example}\label{ex:orthonormalset}
Consider the set of vectors  given by
\[
\left\{ \vec{v}_1, \vec{v}_2 \right\} = \left\{
\begin{bmatrix}
1 \\
1
\end{bmatrix},
\begin{bmatrix}
-1 \\
1
\end{bmatrix}
\right\}
\]
Show that this is an orthogonal set of vectors  but not an orthonormal one. Find the corresponding orthonormal set.

\begin{explanation}
One easily verifies that $\vec{v}_1 \dotp \vec{v}_2 = 0$ and
$\left\{ \vec{v}_1, \vec{v}_2 \right\}$ is an orthogonal set of
vectors. On the other hand one can compute that ${\norm{\vec{v}_1}}= {\norm{\vec{v}_2}} =
\sqrt{2} \neq 1$ and so the set is not orthonormal.

Thus to find a corresponding orthonormal set, we simply need to
normalize each vector. We will write $\{ \vec{u}_1, \vec{u}_2 \}$
for the corresponding orthonormal set. Then,
\begin{eqnarray*}
\vec{u}_1 &=& \frac{1}{\norm{\vec{v}_1}}\vec{v}_1\\
&=& \frac{1}{\sqrt{2}} \begin{bmatrix}
1 \\
1
\end{bmatrix} \\
&=&
\begin{bmatrix}
\frac{1}{\sqrt{2}} \\
\frac{1}{\sqrt{2}}
\end{bmatrix}
\end{eqnarray*}

Similarly,
\begin{eqnarray*}
\vec{u}_2 &=& \frac{1}{\norm{\vec{v}_2}}\vec{v}_2\\
&=& \frac{1}{\sqrt{2}} \begin{bmatrix}
-1 \\
1
\end{bmatrix} \\
&=&
\begin{bmatrix}
-\frac{1}{\sqrt{2}} \\
\frac{1}{\sqrt{2}}
\end{bmatrix}
\end{eqnarray*}

Therefore the corresponding orthonormal set is
\[
\left\{ \vec{u}_1, \vec{u}_2 \right\} =
\left\{
\begin{bmatrix}
\frac{1}{\sqrt{2}} \\
\frac{1}{\sqrt{2}}
\end{bmatrix},
\begin{bmatrix}
-\frac{1}{\sqrt{2}} \\
\frac{1}{\sqrt{2}}
\end{bmatrix}
\right\}
\]

You can verify that this set is orthonormal.
\end{explanation}
\end{example}

\subsection*{Orthogonal and Orthonormal Bases: An intuitive Introduction}
Recall that every basis of $\RR^n$ imposes a coordinate system on $\RR^n$ that can be used to express any vector of $\RR^n$ as a linear combination of the elements of the basis.  For instance,  vectors $\vec{v}_1=\begin{bmatrix}1\\2\end{bmatrix}$ and $\vec{v}_2=\begin{bmatrix}3\\1\end{bmatrix}$ impose a coordinate system on $\RR^2$, as shown below.  We readily see that $\vec{x}=\begin{bmatrix}7\\4\end{bmatrix}$ can be written as $\vec{x}=\vec{v}_1+2\vec{v}_2$.

\begin{center}
\begin{tikzpicture}[scale=1]

  \draw[<->] (-2,0)--(8,0);
  \draw[<->] (0,-2)--(0,6);
\draw[line width=0.5pt, gray](-2,1)--(0.5,6);  
\draw[line width=0.5pt, gray](-1,-2)--(3,6);
\draw[line width=0.5pt, gray](1.5,-2)--(5.5,6);
\draw[line width=0.5pt, gray](4,-2)--(8,6);
\draw[line width=0.5pt, gray](6.5,-2)--(8,1);
\draw[line width=0.5pt, gray](-2, 4.33)--(3,6);
\draw[line width=0.5pt, gray](-2, 2.66)--(8,6);
\draw[line width=0.5pt, gray](-2, 1)--(8,4.33);
\draw[line width=0.5pt, gray](-2, -0.66)--(8,2.66);
\draw[line width=0.5pt, gray](-1, -2)--(8,1);
\draw[line width=0.5pt, gray](4,-2)--(8,-0.66);
 \draw[line width=2pt,blue,-stealth](0,0)--(1,2);
\draw[line width=2pt,red,-stealth](0,0)--(3,1);
\draw[line width=2pt,-stealth](0,0)--(7,4);
\node[blue] at (0.5, 1.5)  (p2)    {$\vec{v}_1$};
\node[red] at (1.6, 0.2)  (p2)    {$\vec{v}_2$};
\node[] at (3.5, 2.3)  (p2)    {$\vec{x}$};
 \end{tikzpicture}
 \end{center}
Vector $\vec{x}$ is easy to work with.  In general, one way to express an arbitrary vector as a linear combination of $\vec{v}_1$ and $\vec{v}_2$ is to solve a system of linear equations, which can be costly.  One reason we like $\{\vec{i},\vec{j}\}$ as a basis of $\RR^2$ is because any vector $\vec{x}$ of $\RR^2$ can be easily expressed as the sum of the orthogonal projections of $\vec{x}$ onto the basis vectors $\vec{i}$ and $\vec{j}$, as shown below.
\begin{center}
\begin{tikzpicture}[scale=1.4]
 \draw[<->] (-1,0)--(3.5,0);
  \draw[<->] (0,-1)--(0,3.5);
  \draw[line width=6pt,-stealth, black!20!white](0,0)--(2,0);
  \draw[line width=6pt,-stealth, black!20!white](0,0)--(0,3);
\draw[line width=2pt,red,-stealth](0,0)--(1,0);  
\draw[line width=2pt,blue,-stealth](0,0)--(0,1);
\draw[line width=2pt,-stealth](0,0)--(2,3);  
\draw[line width=0.5pt,dashed](2,3)--(2,0);  
\draw[line width=0.5pt,dashed](2,3)--(0,3); 
\node[] at (1.7, -0.4)  (p2)    {$\mbox{proj}_{\vec{i}}\vec{x}$};
\node[] at (-0.7, 2.5)  (p2)    {$\mbox{proj}_{\vec{j}}\vec{x}$};
\node[] at (3, 3.1)  (p2)    {$\vec{x}=\mbox{proj}_{\vec{i}}\vec{x}+\mbox{proj}_{\vec{j}}\vec{x}$};
\node[red] at (0.5, -0.3)  (p2)    {$\vec{i}$};
\node[blue] at (-0.3, 0.5)  (p2)    {$\vec{j}$};
 \end{tikzpicture}
\end{center}

We can see why an ``upright" coordinate system with basis $\{\vec{i},\vec{j}\}$ works well.  What if we tilted this coordinate system while preserving the orthogonal relationship between the basis vectors?  The following exploration allows you to investigate the consequences.

\begin{exploration}\label{exp:orth1}
    In the following GeoGebra interactive, vectors $\vec{v}_1$ and $\vec{v}_2$ are orthogonal (slopes of the lines containing them are negative reciprocals of each other).  These vectors are clearly linearly independent and span $\RR^2$.  Therefore they form an orthogonal basis of $\RR^2$.  Let $\vec{x}$ be an arbitrary vector.  Orthogonal projections of $\vec{x}$ onto $\vec{v}_1$ and $\vec{v}_2$ are depicted in light grey.
    \begin{enumerate}
        \item Use the tip of vector $\vec{x}$ to manipulate the vector and convince yourself that $\vec{x}$ is always the diagonal of the parallelogram (a rectangle!) determined by the projections.
        \item Use the tips of $\vec{v}_1$ and $\vec{v}_2$ to change the basis vectors.  What happens when $\vec{v}_1$ and $\vec{v}_2$ are no longer orthogonal?
        \item Pick another pair of orthogonal vectors $\vec{v}_1$ and $\vec{v}_2$.  Verify that $\vec{x}$ is still the sum of the projections.
    \end{enumerate}

    \begin{center}
\geogebra{nsqzhsxv}{800}{600}
\end{center}
\end{exploration}

\subsection*{Orthogonal and Orthonormal Bases}
Consider an orthogonal set of vectors in $\RR^n$, written $\{
\vec{w}_1, \cdots, \vec{w}_k \}$ with $k \leq n$. The span of these
vectors is a subspace $W$ of $\RR^n$.   If we
could show that this orthogonal set is also linearly independent, we
would have a basis of $W$. This is precisely what we do in the following theorem.

\begin{theorem}[Orthogonal Basis of a Subspace]\label{orthbasis}
Let $ \{ \vec{w}_1, \vec{w}_2, \cdots, \vec{w}_k \}$ be an
orthogonal set of vectors in $\RR^n$. Then this set is
linearly independent and forms a basis for the subspace $W =
\mbox{span}\left( \vec{w}_1, \vec{w}_2, \cdots, \vec{w}_k \right)$.
\end{theorem}

\begin{proof}
To show this set is linearly independent, suppose a linear combination
of these vectors equals $\vec{0}$, such as:
\[
a_1 \vec{w}_1 + a_2 \vec{w}_2 + \cdots + a_k \vec{w}_k = \vec{0}, a_i \in \RR
\]
We need to show that all $a_i = 0$. To do so, we take the dot product of
each side of the above equation with the vector $\vec{w}_i$ and obtain the following.

\begin{eqnarray*}
\vec{w}_i \dotp (a_1 \vec{w}_1 + a_2 \vec{w}_2 + \cdots + a_k \vec{w}_k ) &=& \vec{w}_i \dotp \vec{0}\\
a_1 (\vec{w}_i \dotp \vec{w}_1) + a_2 (\vec{w}_i \dotp \vec{w}_2) + \cdots + a_k (\vec{w}_i \dotp \vec{w}_k)  &=& 0
\end{eqnarray*}
Now since the set is orthogonal, $\vec{w}_i \dotp \vec{w}_m = 0$ for all $m \neq i$, so we have:
\[
a_1 (0) + \cdots + a_i(\vec{w}_i \dotp \vec{w}_i) + \cdots + a_k (0) = 0
\]
\[
a_i \norm{\vec{w}_i}^2 = 0
\]

Since the set is orthogonal, we know that $\norm{\vec{w}_i}^2  \neq 0$. It follows that $a_i =0$. Since the $a_i$ was chosen arbitrarily, the set $\{ \vec{w}_1, \vec{w}_2, \cdots, \vec{w}_k \}$ is linearly independent.

Finally since $W = \mbox{span} \{ \vec{w}_1, \vec{w}_2, \cdots,
\vec{w}_k \}$, the set of vectors also spans $W$ and therefore forms a basis of $W$.

\end{proof}

If an orthogonal set is a basis for a subspace, we call it an
\dfn{orthogonal basis}. Similarly, if an orthonormal set is a basis, we call it an \dfn{orthonormal basis}.

In Exploration \ref{exp:orth1} you have found that we can express an arbitrary vector of $\RR^2$ as the sum of its projections onto the basis vectors, provided that the basis is orthogonal. It turns out that this result holds in any subspace of $\RR^n$. The following theorem proves this result.  As you read the statement of the theorem, it will be helpful to recall that the orthogonal projection of vector $\vec{x}$ onto a non-zero vector $\vec{d}$ is given by
$$\mbox{proj}_{\vec{d}}\vec{x}=\left(\frac{\vec{x}\cdot\vec{d}}{\norm{\vec{d}}^2}\right)\vec{d}$$ (See \href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/VEC-0070/main}{Section ???} on orthogonal projections.)

\begin{theorem}\label{fourierexpansion}
Let $V$ be a subspace of $\RR^n$ and suppose $\{ \vec{v}_1, \vec{v}_2, \ldots, \vec{v}_m \}$
is an orthogonal basis of $V$.
Then we can express any $\vec{x}\in V$ as a linear combination of the basis elements,
\[ \vec{x} =
c_1 \vec{v}_1 +
c_2 \vec{v}_2 +
\cdots +
c_m \vec{v}_m,
\]
and, in fact, $c_i = \frac{\vec{x}\dotp \vec{v}_i}{\norm{\vec{v}_i}^2}$ for $i=1,\ldots,m$, so that
\begin{equation}
\vec{x} =
\left(\frac{\vec{x}\dotp \vec{v}_1}{\norm{\vec{v}_1}^2}\right) \vec{v}_1 +
\left(\frac{\vec{x}\dotp \vec{v}_2}{\norm{\vec{v}_2}^2}\right) \vec{v}_2 +
\cdots +
\left(\frac{\vec{x}\dotp \vec{v}_m}{\norm{\vec{v}_m}^2}\right) \vec{v}_m.
\end{equation}\label{FourierEqn}
%This expression is called the Fourier expansion
%of $\vec{x}$, and
%\[ \frac{\vec{x}\dotp \vec{v}_j}{\norm{\vec{v}_j}^2},\]
%$j=1,2,\ldots,m$
%are the Fourier coefficients.
\end{theorem}

\begin{proof}
The first statement in the theorem is clear, since any basis is also a spanning set.  

We need to establish the formula $c_i = \frac{\vec{x}\dotp \vec{v}_i}{\norm{\vec{v}_i}^2}$ for $i=1,\ldots,m$. To do so, we take the dot product of
each side with the vector $\vec{v}_i$ and obtain the following.

\begin{equation*}
  \vec{x} \dotp \vec{v}_i =  \left(c_1\vec{v}_1 +
c_2\vec{v}_2 +
\cdots +
c_m\vec{v}_m\right) \dotp \vec{v}_i 
\end{equation*}
Our basis is orthogonal, so $\vec{v}_j \dotp \vec{v}_i = 0$ for all $j \neq i$, which means after we distribute the dot product, only one term will remain on the right-hand side.  We have 
\begin{equation*}
  \vec{x} \dotp \vec{v}_i =  c_i\vec{v}_i \dotp \vec{v}_i 
\end{equation*}

We now divide both sides by $\vec{v}_i \dotp \vec{v}_i = \norm{\vec{v}_i}^2$, and since $i$ was arbitrary, the proof is complete.
\end{proof}

Theorem~\ref{fourierexpansion} shows one important benefit of a basis being orthogonal.  With an orthogonal basis it is easy to represent any vector in terms of the basis vectors.  

\begin{example}\label{fourier}
Let
$\vec{v}_1= \begin{bmatrix}
1 \\ -1 \\ 2
\end{bmatrix},
\vec{v}_2= \begin{bmatrix}
0 \\ 2 \\ 1 
\end{bmatrix},
\vec{v}_3 =\begin{bmatrix}
5 \\ 1 \\ -2
\end{bmatrix}$,
and let
$\vec{x} =\begin{bmatrix}
1 \\ 1 \\ 1
\end{bmatrix}$.  

Notice that $\mathcal{B}=\{ \vec{v}_1, \vec{v}_2, \vec{v}_3\}$
is an \underline{orthogonal} set of vectors.  Use this fact to write $\vec{x}$ as  a linear combination of the vectors of $\mathcal{B}$.

\begin{explanation}
We first apply Theorem~\ref{orthbasis} to say that $\mathcal{B}$ is linearly independent set of vectors, and so $\mathcal{B}$ forms a basis for $\RR^3$. Next we apply Theorem~\ref{fourierexpansion} to express $\vec{x}$ as  a linear combination of the vectors of $\mathcal{B}$.  We wish to write:

\[
\vec{x}   =
\left(\frac{\vec{x}\dotp \vec{v}_1}{\norm{\vec{v}_1}^2}\right) \vec{v}_1 +
\left(\frac{\vec{x}\dotp \vec{v}_2}{\norm{\vec{v}_2}^2}\right) \vec{v}_2 +
\left(\frac{\vec{x}\dotp \vec{v}_3}{\norm{\vec{v}_3}^2}\right) \vec{v}_3.
\]

We readily compute:

\[
\frac{\vec{x}\dotp\vec{v}_1}{\norm{\vec{v}_1}^2} = \frac{2}{6}, \;
\frac{\vec{x}\dotp\vec{v}_2}{\norm{\vec{v}_2}^2} = \frac{3}{5},
\mbox{ and }
\frac{\vec{x}\dotp\vec{v}_3}{\norm{\vec{v}_3}^2} = \frac{4}{30}.\]

Therefore,
\[ \begin{bmatrix}
1 \\ 1 \\ 1
\end{bmatrix}
= \frac{1}{3}\begin{bmatrix}
1 \\ -1 \\ 2
\end{bmatrix}
+\frac{3}{5}\begin{bmatrix}
0 \\ 2 \\ 1
\end{bmatrix}
+\frac{2}{15}\begin{bmatrix}
5 \\ 1 \\ -2
\end{bmatrix}.\]
\end{explanation} 
\end{example}

The formula from Theorem~\ref{fourier} is easy to use, and it becomes even easier when our basis is \emph{orthonormal}.

\begin{corollary}
Let $V$ be a subspace of $\RR^n$ and suppose $\{ \vec{u}_1, \vec{u}_2, \ldots, \vec{u}_m \}$
is an orthonormal basis of $V$.
Then for any $\vec{x}\in V$,
\[ \vec{x} =
\left(\vec{x}\dotp \vec{u}_1\right) \vec{u}_1 +
\left(\vec{x}\dotp \vec{u}_2\right) \vec{u}_2 +
\cdots +
\left(\vec{x}\dotp \vec{u}_m\right)  \vec{u}_m.
\]
\end{corollary}
\begin{proof}
This is a special case of Theorem \ref{fourierexpansion} where we can compute the coefficients of $x$ with respect to the basis by simply taking the dot product with each basis vector, for in this case $\norm{\vec{u_i}} = 1$ for $i=1,\ldots,m$.
\end{proof}

\section*{Practice Problems}
\begin{problem}\label{prob:rref_way}
Retry Example~\ref{fourier} using Gaussian elimination.  Is the method of Example~\ref{fourier} more efficient?
\end{problem}
\begin{problem}\label{prob:vec_eq_0}
    Let $\vec{x}_1, \vec{x}_2, \ldots, \vec{x}_k\in\RR^n$ and
suppose $\mbox{span}\{\vec{x}_1, \vec{x}_2, \ldots, \vec{x}_k\}=\RR^n$.
Furthermore, suppose that there exists a vector $\vec{v}\in\RR^n$ for which $\vec{v}\dotp \vec{x}_j=0$ for all $j$, $1\leq j\leq k$.
Show that $\vec{v}=\vec{0}$.
\end{problem}
  
\section*{Text Source}
A portion of the text in this module is an adaptation of Section 4.11.1 of Ken Kuttler's \href{https://open.umn.edu/opentextbooks/textbooks/a-first-course-in-linear-algebra-2017}{\it A First Course in Linear Algebra}. (CC-BY)

Ken Kuttler, {\it  A First Course in Linear Algebra}, Lyryx 2017, Open Edition, p. 233-238.  

\end{document}
