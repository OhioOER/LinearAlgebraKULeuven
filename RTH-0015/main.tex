\documentclass{ximera}
\input{../preamble.tex}

\author{Paul Zachlin \and Anna Davis} \title{Orthogonal Matrices} \license{CC-BY 4.0}

\begin{document}

\begin{abstract}

\end{abstract}
\maketitle

\section*{Orthogonal Matrices}
Recall that the process to find the inverse of a matrix is often cumbersome.
In contrast, it is very easy to take the transpose of a matrix. Luckily for some special matrices, the transpose equals the inverse. When an $n \times n$ matrix with real
entries has its transpose equal to its inverse, the matrix is called an \dfn{orthogonal matrix}.

The precise definition is as follows.

\begin{definition}\label{OrthoMatrix}
A real $n\times n$ matrix $U$ is called an \dfn{orthogonal} matrix if $UU^{T}=U^{T}U=I.$
\end{definition}

Note that since $U$ is assumed to be a square matrix, it suffices to verify only one of these equalities $UU^{T}=I$ or $U^{T}U=I$ holds in order to guarantee that $U^T$ is the inverse of $U$.

Consider the following example.

\begin{example}\label{ex:OrthogonalMatrix1}
Show the matrix
\begin{equation*}
U=\begin{bmatrix}
\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \\
\frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}}
\end{bmatrix}
\end{equation*}
is orthogonal.
\end{example}

\begin{explanation}
All we need to do is verify (one of the equations from) the requirements of Definition \ref{OrthoMatrix}.

\begin{equation*}
UU^{T}=\begin{bmatrix}
\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \\
\frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}}
\end{bmatrix} \begin{bmatrix}
\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \\
\frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}}
\end{bmatrix} = \begin{bmatrix}
1 & 0 \\
0 & 1
\end{bmatrix}
\end{equation*}

Since $UU^{T} = I$, this matrix is orthogonal.
\end{explanation}

\begin{example}\label{ex:OrthogonalMatrix2}
Let $U=\begin{bmatrix}
1 & 0 & 0 \\
0 & 0 & -1 \\
0 & -1 & 0
\end{bmatrix}.$ Is $U$ orthogonal?
\end{example}

\begin{explanation}
Again the answer is yes and this can be verified simply by showing that $U^{T}U=I$:

\begin{eqnarray*}
U^{T}U&=&\begin{bmatrix}
1 & 0 & 0 \\
0 & 0 & -1 \\
0 & -1 & 0
\end{bmatrix}^{T}\begin{bmatrix}
1 & 0 & 0 \\
0 & 0 & -1 \\
0 & -1 & 0
\end{bmatrix} \\
&=&\begin{bmatrix}
1 & 0 & 0 \\
0 & 0 & -1 \\
0 & -1 & 0
\end{bmatrix}
\begin{bmatrix}
1 & 0 & 0 \\
0 & 0 & -1 \\
0 & -1 & 0
\end{bmatrix} \\
&=&\begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix}
\end{eqnarray*}
\end{explanation}

When we say that $U$ is orthogonal, we are saying that $UU^T=I$, meaning that
\begin{equation*}
\sum_{j}u_{ij}u_{jk}^{T}=\sum_{j}u_{ij}u_{kj}=\delta _{ik}
\end{equation*}
where $\delta _{ij}$ is the \textbf{Kronecker symbol}
defined
\index{Kronecker symbol} by
\begin{equation*}
\delta _{ij}=\left\{
\begin{array}{c}
1
\text{ if }i=j \\
0\text{ if }i\neq j
\end{array}
\right.
\end{equation*}

These equations say that the product of the $i^{th}$ row of $U$ with the $k^{th}$ row
gives $1$ if $i=k$ and $0$ if $i\neq k.$ The same is true of the columns because
$U^{T}U=I$ also. Therefore,
\begin{equation*}
\sum_{j}u_{ij}^{T}u_{jk}=\sum_{j}u_{ji}u_{jk}=\delta _{ik}
\end{equation*}
which says that the product of one column with another column gives $1$ if the two
columns are the same and $0$ if the two columns are different.

More succinctly, this states that if $\vec{u}_{1},\cdots ,\vec{u}_{n}$
are the columns of $U,$ an orthogonal matrix, then
\[
\vec{u}_{i}\dotp \vec{u}_{j}=\delta _{ij} = \left\{
\begin{array}{c}
1\text{ if }i=j \\
0\text{ if }i\neq j
\end{array}
\right.
\]

We will say that the columns form an orthonormal set of vectors, and similarly for the rows. Thus a matrix is \dfn{orthogonal} if its rows (or columns) form an
\dfn{orthonormal} set of vectors. Notice that the convention is to call such a matrix orthogonal rather than orthonormal (although perhaps the latter term would make more sense!).

\begin{theorem}[Orthonormal Basis]\label{orthbasis}
The rows of an $n \times n$ orthogonal matrix form an orthonormal
basis of $\RR^n$. Furthermore, any orthonormal basis of
$\RR^n$ can be used to construct an $n \times n$ orthogonal
matrix.
\end{theorem}

\begin{proof}
Recall from Theorem \ref{orthbasis} that an orthonormal set is
linearly independent and forms a basis for its span. Since the rows of
an $n \times n$ orthogonal matrix form an orthonormal set, they must
be linearly independent. Now we have $n$ linearly independent vectors,
and it follows that their span equals $\mathbb{R}^n$. Therefore these
vectors form an orthonormal basis for $\mathbb{R}^n$.

Suppose now that we have an orthonormal basis for $\mathbb{R}^n$. Since the
basis will contain $n$ vectors, these can be used to construct an $n
\times n$ matrix, with each vector becoming a row. Therefore the
matrix is composed of orthonormal rows, which by our above discussion,
means that the matrix is orthogonal. Note we could also have construct
a matrix with each vector becoming a column instead, and this would
again be an orthogonal matrix. In fact this is simply the transpose of
the previous matrix.
\end{proof}

Consider the following proposition.

\begin{theorem}\label{orthoDet}
Suppose $U$ is an orthogonal matrix. Then $\det \left( U\right) = \pm 1.$
\end{theorem}

\begin{proof}
This result follows from the properties of determinants. Recall that
for any matrix $A$, $\det(A)^T = \det(A)$. Now if $U$ is orthogonal, then:
\begin{equation*}
(\det \left( U\right)) ^{2}=\det \left( U^{T}\right) \det \left( U\right)
=\det \left( U^{T}U\right) =\det \left( I\right) =1
\end{equation*}

Therefore $(\det (U))^2 = 1$ and it follows that $\det \left( U\right) = \pm 1$.
\end{proof}

Orthogonal matrices are divided into two classes, proper and improper.
The proper orthogonal matrices are those whose determinant equals 1
and the improper ones are those whose determinant equals $-1$. The
reason for the distinction is that the improper orthogonal matrices
are sometimes considered to have no physical significance. These
matrices cause a change in orientation which would correspond to
material passing through itself in a non physical manner. Thus in
considering which coordinate systems must be considered in certain
applications, you only need to consider those which are related by a
proper orthogonal transformation. Geometrically, the linear
transformations determined by the proper orthogonal matrices
correspond to the composition of rotations.

We conclude this section with two more useful properties of orthogonal matrices.

\begin{theorem}{Product and Inverse of Orthogonal Matrices}{productinverseorthogonal}
Suppose $A$ and $B$ are orthogonal matrices. Then $AB$ and $A^{-1}$ both exist and are orthogonal.
\end{theorem}

\begin{proof}
First we examine the product $AB$.
\[ (AB)(B^TA^T)=A(BB^T)A^T =AA^T=I \]
Since $AB$ is square, $B^TA^T=(AB)^T$ is the inverse of
$AB$, so $AB$ is invertible, and $(AB)^{-1}=(AB)^T$
Therefore, $AB$ is orthogonal.

Next we show that $A^{-1}=A^T$ is also orthogonal.
\[ (A^{-1})^{-1} = A = (A^T)^{T}
=(A^{-1})^{T} \]
Therefore $A^{-1}$ is also orthogonal.
\end{proof} 
\section*{Practice Problems}
\begin{problem}\label{prob:rotationsRorthogonal}
Recall that \begin{equation} 
M_{\theta}=\begin{bmatrix}
\cos\theta & -\sin\theta\\
\sin\theta & \cos\theta
\end{bmatrix}
\end{equation}
induces a counterclockwise rotation in $\RR^2$ by $\theta$ about the origin.  Show that this matrix is an orthogonal matrix.
\end{problem}
  
\section*{Text Source}
A large portion of the text in this module is an adaptation of Section 4.11.2 of Ken Kuttler's \href{https://open.umn.edu/opentextbooks/textbooks/a-first-course-in-linear-algebra-2017}{\it A First Course in Linear Algebra}. (CC-BY)

Ken Kuttler, {\it  A First Course in Linear Algebra}, Lyryx 2017, Open Edition, p. 238-241.  

\end{document}
