\documentclass{ximera}
\input{../preamble.tex}

\author{Paul Zachlin \and Anna Davis} \title{Gram-Schmidt Orthogonalization} \license{CC-BY 4.0}

\begin{document}

\begin{abstract}

\end{abstract}
\maketitle

\section*{Orthogonal Complements and Projections}\label{sec:8_1}

%%%%%%%% Anna's suggestion for an intuitive intro - this is just a sketch.
\subsection*{Creating an Orthogonal Basis}
We know that orthogonal bases are nice.  Does every subspace of $\RR^n$ have one?  Can we take an arbitrary basis as our starting point and construct an orthogonal basis from it?  Let's take a look at $\RR^2$.  Given an arbitrary basis $\{\vec{x}_1, \vec{x}_2\}$, let's start building our orthogonal basis $\{\vec{v}_1, \vec{v}_2\}$ by setting $\vec{v}_1 = \vec{x}_1$.To find the next element of our orthogonal basis $\vec{v}_2$, consider the orthogonal projection of $\vec{x}_2$ onto $\vec{v}_1$.  Let $\vec{v}_2=\vec{x}_2-\mbox{proj}_{\vec{v}_1}\vec{x}_2$.  From the picture we can see that $\vec{v}_2$ is orthogonal to $\vec{v}_1$.  We will take $\vec{v}_2$ as the second vector in our orthogonal basis.  Let $\mathcal{B}=\{\vec{v}_1,\vec{v}_2\}$.  By Theorem ??? , $\vec{v}_1$ and $\vec{v}_2$ are linearly independent.  Since we are in $\RR^2$, $\mathcal{B}$ only has two elements.  This completes the process of constructing an orthogonal basis.
\begin{center}
\begin{tikzpicture}[scale=1.2]
 \draw[<->] (-1,0)--(5,0);
  \draw[<->] (0,-1)--(0,5);
  \draw[line width=6pt,-stealth, black!20!white](0,0)--(2,4);
\draw[line width=2pt,red,-stealth](0,0)--(1,2);  
\draw[line width=2pt,blue,-stealth](0,0)--(4,3);
\draw[line width=2pt,-stealth](2,4)--(4,3);  
\node[] at (0.7, 2.8)  (p2)    {$\mbox{proj}_{\vec{v}_1}\vec{v}_2$};
\node[] at (3.9, 3.8)  (p2)    {$\vec{f}=\vec{v}_2-\mbox{proj}_{\vec{v}_1}\vec{v}_2$};
\node[red] at (0.3, 1)  (p2)    {$\vec{v}_1$};
\node[blue] at (2, 1.2)  (p2)    {$\vec{v}_2$};
 \end{tikzpicture}
\end{center}

Next we turn our attention to $\RR^3$.

Here are some geogebras just so we can see what they look like:
\begin{exploration}
Here are some manipulatives:
\begin{center}
\geogebra{qjpvmsws}{900}{800}
\end{center}

\begin{center}
\geogebra{xtqppyav}{800}{600}
\end{center}

\begin{center}
\geogebra{zghsfkym}{800}{600}
\end{center}
\end{exploration}




\subsection*{Gram-Schmidt Orthogonalization Algorithm}
If $\{\vec{v}_{1}, \dots , \vec{v}_{m}\}$ is linearly independent in a general vector space, and if $\vec{v}_{m+1}$ is not in $\mbox{span}\left(\vec{v}_{1}, \dots , \vec{v}_{m}\right)$, then $\{\vec{v}_{1}, \dots , \vec{v}_{m}, \vec{v}_{m+1}\}$ is also linearly independent (See Problem \ref{prob:Adding1OK} in our section on \href{https://ximera.osu.edu/oerlinalg/LinearAlgebra/VEC-0100/main}{Linear Independence}). Here is the analog for \dfn{orthogonal} sets in $\RR^n$.


\begin{lemma}[Orthogonal Lemma]\label{023597}
Let $\{\vec{f}_{1}, \vec{f}_{2}, \dots , \vec{f}_{m}\}$ be an orthogonal set in $\RR^n$. Given $\vec{x}$ in $\RR^n$, write
\begin{equation*}
\vec{f}_{m+1} = \vec{x} - \frac{\vec{x} \dotp \vec{f}_{1}}{\norm{\vec{f}_{1}}^2}\vec{f}_{1} - \frac{\vec{x} \dotp \vec{f}_{2}}{\norm{\vec{f}_{2}}^2}\vec{f}_{2} - \dots - \frac{\vec{x} \dotp \vec{f}_{m}}{\norm{\vec{f}_{m}}^2}\vec{f}_{m}
\end{equation*}
Then:

\begin{enumerate}
\item\label{023597a} $\vec{f}_{m+1} \dotp \vec{f}_{k} = 0$ for $k = 1, 2, \dots , m$.

\item\label{023597b} If $\vec{x}$ is not in $\mbox{span}\left(\vec{f}_{1}, \dots , \vec{f}_{m}\right)$, then $\vec{f}_{m+1} \neq \vec{0}$ and $\{\vec{f}_{1}, \dots , \vec{f}_{m}, \vec{f}_{m+1}\}$ is an orthogonal set.

\end{enumerate}
\end{lemma}

\begin{proof}
For convenience, write $c_{i} = (\vec{x} \dotp \vec{f}_{i}) / \norm{\vec{f}_{i}}^{2}$ for each $i$. Given $1 \leq k \leq m$:
\begin{align*}
\vec{f}_{m+1} \dotp \vec{f}_{k}
&= (\vec{x} - c_{1}\vec{f}_{1} - \dots -c_{k}\vec{f}_{k} - \dots -c_{m}\vec{f}_{m}) \dotp \vec{f}_{k} \\
&= \vec{x} \dotp \vec{f}_{k} - c_{1}(\vec{f}_{1} \dotp \vec{f}_{k}) - \dots -c_{k}(\vec{f}_{k} \dotp \vec{f}_{k}) - \dots -c_{m}(\vec{f}_{m} \dotp \vec{f}_{k}) \\
&= \vec{x} \dotp \vec{f}_{k} - c_{k} \norm{\vec{f}_{k}}^2 \\
&= 0
\end{align*}
This proves (1), and (2) follows because $\vec{f}_{m + 1} \neq \vec{0}$ if $\vec{x}$ is not in $\mbox{span}\left(\vec{f}_{1}, \dots , \vec{f}_{m}\right)$.
\end{proof}

The orthogonal lemma has three important consequences for $\RR^n$. The first is an extension for orthogonal sets of the fundamental fact that any independent set is part of a basis (Theorem~\ref{019430}).


\begin{theorem}\label{023635}
Let $W$ be a subspace of $\RR^n$.


\begin{enumerate}
\item\label{023635a} Every orthogonal subset $\{\vec{f}_{1}, \dots , \vec{f}_{m}\}$ in $W$ is a subset of an orthogonal basis of $W$.

\item\label{023635b} $W$ has an orthogonal basis.

\end{enumerate}
\end{theorem}



\begin{proof}
\begin{enumerate}
\item If $\mbox{span}\left(\vec{f}_{1}, \dots , \vec{f}_{m}\right) = U$, it is \textit{already} a basis. Otherwise, there exists $\vec{x}$ in $W$ outside $\mbox{span}\left(\vec{f}_{1}, \dots , \vec{f}_{m}\right)$. If $\vec{f}_{m+1}$ is as given in the orthogonal lemma, then $\vec{f}_{m+1}$ is in $W$ and $\{\vec{f}_{1}, \dots , \vec{f}_{m}, \vec{f}_{m+1}\}$ is orthogonal. If $\mbox{span}\left(\vec{f}_{1}, \dots, \vec{f}_{m}, \vec{f}_{m+1}\right) = U$, we are done. Otherwise, the process continues to create larger and larger orthogonal subsets of $W$. They are all independent by Theorem~\ref{015056}, so we have a basis when we reach a subset containing dim $W$ vectors.

\item If $W = \{\vec{0}\}$, the empty basis is orthogonal. Otherwise, if $\vec{f} \neq \vec{0}$ is in $W$, then $\{\vec{f}\}$ is orthogonal, so (2) follows from (1).
\end{enumerate}
\end{proof}

We can improve upon \ref{023635b} of Theorem~\ref{023635}. In fact, the second consequence of the orthogonal lemma is a procedure by which \textit{any} basis $\{\vec{x}_{1}, \dots , \vec{x}_{m}\}$ of a subspace $W$ of $\RR^n$ can be systematically modified to yield an orthogonal basis $\{\vec{f}_{1}, \dots , \vec{f}_{m}\}$ of $W$. The $\vec{f}_{i}$ are constructed one at a time from the $\vec{x}_{i}$.

To start the process, take $\vec{f}_{1} = \vec{x}_{1}$. Then $\vec{x}_{2}$ is not in $\mbox{span}\left(\vec{f}_{1}\right)$ because $\{\vec{x}_{1}, \vec{x}_{2}\}$ is independent, so take
\begin{equation*}
\vec{f}_{2} = \vec{x}_{2} - \frac{\vec{x}_{2} \dotp \vec{f}_{1}}{\norm{\vec{f}_{1}}^2}\vec{f}_{1}
\end{equation*}
Thus $\{\vec{f}_{1}, \vec{f}_{2}\}$ is orthogonal by Lemma \ref{lem:023597}. Moreover, $\mbox{span}\left(\vec{f}_{1}, \vec{f}_{2}\right) = \mbox{span}\left(\vec{x}_{1}, \vec{x}_{2}\right)$ (verify), so $\vec{x}_{3}$ is not in $\mbox{span}\left(\vec{f}_{1}, \vec{f}_{2}\right)$. Hence $\{\vec{f}_{1}, \vec{f}_{2}, \vec{f}_{3}\}$ is orthogonal where
\begin{equation*}
\vec{f}_{3} = \vec{x}_{3} - \frac{\vec{x}_{3} \dotp \vec{f}_{1}}{\norm{\vec{f}_{1}}^2}\vec{f}_{1} - \frac{\vec{x}_{3} \dotp \vec{f}_{2}}{\norm{\vec{f}_{2}}^2}\vec{f}_{2}
\end{equation*}
Again, $\mbox{span}\left(\vec{f}_{1}, \vec{f}_{2}, \vec{f}_{3}\right) = \mbox{span}\left(\vec{x}_{1}, \vec{x}_{2}, \vec{x}_{3}\right)$, so $\vec{x}_{4}$ is not in $\mbox{span}\left(\vec{f}_{1}, \vec{f}_{2}, \vec{f}_{3}\right)$ and the process continues. At the $m$th iteration we construct an orthogonal set $\{\vec{f}_{1}, \dots , \vec{f}_{m}\}$ such that
\begin{equation*}
\mbox{span}\left(\vec{f}_{1}, \vec{f}_{2}, \dots, \vec{f}_{m}\right) = \mbox{span}\left(\vec{x}_{1}, \vec{x}_{2}, \dots, \vec{x}_{m}\right) = U
\end{equation*}
Hence $\{\vec{f}_{1}, \vec{f}_{2}, \dots , \vec{f}_{m}\}$ is the desired orthogonal basis of $W$. The procedure can be summarized as follows.

%\newpage
%\begin{wrapfigure}[4]{l}{5cm}
%\vspace*{5em}
%\centering
%\input{8-orthogonality/figures/1-orthogonal-complements-and-projections/theorem8.1.2}
%\caption{\label{fig:023740}}
%\end{wrapfigure}
%\hfill
\begin{theorem}[Gram-Schmidt Orthogonalization] \label{023713}
If $\{\vec{x}_{1}, \vec{x}_{2}, \dots , \vec{x}_{m}\}$ is any basis of a subspace $W$ of $\RR^n$, construct $\vec{f}_{1}, \vec{f}_{2}, \dots , \vec{f}_{m}$ in $W$ successively as follows:
\begin{equation*}
\begin{array}{ccl}
\vec{f}_{1} &=& \vec{x}_{1} \\
\vec{f}_{2} &=& \vec{x}_{2} - \frac{\vec{x}_{2} \dotp \vec{f}_{1}}{\norm{\vec{f}_{1}}^2}\vec{f}_{1} \\
\vec{f}_{3} &=& \vec{x}_{3} - \frac{\vec{x}_{3} \dotp \vec{f}_{1}}{\norm{\vec{f}_{1}}^2}\vec{f}_{1} - \frac{\vec{x}_{3} \dotp \vec{f}_{2}}{\norm{\vec{f}_{2}}^2}\vec{f}_{2} \\
\vdots &&\\
\vec{f}_{k} &=& \vec{x}_{k} - \frac{\vec{x}_{k} \dotp \vec{f}_{1}}{\norm{\vec{f}_{1}}^2}\vec{f}_{1} - \frac{\vec{x}_{k} \dotp \vec{f}_{2}}{\norm{\vec{f}_{2}}^2}\vec{f}_{2} - \dots -\frac{\vec{x}_{k} \dotp \vec{f}_{k-1}}{\norm{\vec{f}_{k-1}}^2}\vec{f}_{k-1}
\end{array}
\end{equation*}
for each $k = 2, 3, \dots , m$. Then
\begin{enumerate}
\item $\{\vec{f}_{1}, \vec{f}_{2}, \dots , \vec{f}_{m}\}$ is an orthogonal basis of $W$.

\item $\mbox{span}\left(\vec{f}_{1}, \vec{f}_{2}, \dots , \vec{f}_{k}\right) = \mbox{span}\left(\vec{x}_{1}, \vec{x}_{2}, \dots , \vec{x}_{k}\right)$ for each $k = 1, 2, \dots , m$.

\end{enumerate}\index{Gram-Schmidt orthogonalization algorithm}\index{orthogonality!Gram-Schmidt orthogonalization algorithm}
\end{theorem}

(Note: Erhardt
 Schmidt (1876--1959) was a German mathematician who studied under the
great David Hilbert. He
 first described the present algorithm in 1907. J\"{o}rgen Pederson Gram
(1850--1916)\index{Gram, J\"{o}rgen Pederson}  was a Danish actuary.)


The process (for $k = 3$) is depicted in the diagrams. Of course, the algorithm converts any basis of $\RR^n$ itself into an orthogonal basis.


\begin{example}\label{023743}
Find an orthogonal basis of the row space of $A = \begin{bmatrix}
1 & 1 & -1 & -1\\
3 & 2 & 0 & 1\\
1 & 0 & 1 & 0
\end{bmatrix}$.

\begin{explanation}
  Let $\vec{x}_{1}$, $\vec{x}_{2}$, $\vec{x}_{3}$ denote the rows of $A$ and observe that $\{\vec{x}_{1}, \vec{x}_{2}, \vec{x}_{3}\}$ is linearly independent. Take $\vec{f}_{1} = \vec{x}_{1}$. The algorithm gives
\begin{align*}
\vec{f}_{2} &= \vec{x}_{2} - \frac{\vec{x}_{2} \dotp \vec{f}_{1}}{\norm{\vec{f}_{1}}^2}\vec{f}_{1} = (3, 2, 0, 1) - \frac{4}{4}(1, 1, -1, -1) = (2, 1, 1, 2) \\
\vec{f}_{3} &= \vec{x}_{3} - \frac{\vec{x}_{3} \dotp \vec{f}_{1}}{\norm{\vec{f}_{1}}^2}\vec{f}_{1} - \frac{\vec{x}_{3} \dotp \vec{f}_{2}}{\norm{\vec{f}_{2}}^2}\vec{f}_{2} = \vec{x}_{3} - \frac{0}{4}\vec{f}_{1} - \frac{3}{10}\vec{f}_{2} = \frac{1}{10}(4, -3, 7, -6)
\end{align*}

Hence $\{(1, 1, -1, -1), (2, 1, 1, 2), \frac{1}{10}(4, -3, 7, -6)\}$ is the orthogonal basis provided by the algorithm. In
hand calculations it may be convenient to eliminate fractions (see the Remark below), so $\{(1, 1, -1, -1), (2, 1, 1, 2), (4, -3, 7, -6)\}$ is also an orthogonal basis for row $A$.
\end{explanation}
\end{example}

\textbf{Remark}
Observe that the vector $\frac{\vec{x} \dotp \vec{f}_{i}}{\norm{\vec{f}_{i}}^2}\vec{f}_{i}$
 is unchanged if a nonzero scalar multiple of $\vec{f}_{i}$ is used in place of $\vec{f}_{i}$. Hence, if a newly constructed $\vec{f}_{i}$ is multiplied by a nonzero scalar at some stage of the Gram-Schmidt algorithm, the subsequent $\vec{f}$s will be unchanged. This is useful in actual calculations.

%WHERE DOES THIS EXAMPLE GO?
\begin{example}\label{023908}
Let $W = \mbox{span}\left(\vec{x}_{1}, \vec{x}_{2}\}\right)$ in $\RR^4$ where $\vec{x}_{1} = \begin{bmatrix}
1 \\ 1 \\ 0 \\ 1
\end{bmatrix}$ and $\vec{x}_{2} = \begin{bmatrix}
0 \\ 1 \\ 1 \\ 2
\end{bmatrix}$. If $\vec{x} = \begin{bmatrix}
3 \\ -1 \\ 0 \\ 2
\end{bmatrix}$, find the vector in $W$ closest to $\vec{x}$ and express $\vec{x}$ as the sum of a vector in $W$ and a vector orthogonal to $W$.

\begin{explanation}
  $\{\vec{x}_{1}, \vec{x}_{2}\}$ is independent but not orthogonal. The Gram-Schmidt process gives an orthogonal basis $\{\vec{f}_{1}, \vec{f}_{2}\}$ of $W$ where $\vec{f}_{1} = \vec{x}_{1} = \begin{bmatrix}
1 \\ 1 \\ 0 \\ 1
\end{bmatrix}$ and
\begin{equation*}
\vec{f}_{2} =\vec{x}_{2} - \frac{\vec{x}_{2} \dotp \vec{f}_{1}}{\norm{\vec{f}_{1}}^2}\vec{f}_{1} = \vec{x}_{2} - \frac{3}{3}\vec{f}_{1} = \vec{x}_{1} = \begin{bmatrix}
-1 \\ 0 \\ 1 \\ 1
\end{bmatrix}
\end{equation*}
Hence, we can compute the projection using $\{\vec{f}_{1}, \vec{f}_{2}\}$:
\begin{equation*}
\vec{p} = \mbox{proj}_W(\vec{x}) =\frac{\vec{x} \dotp \vec{f}_{1}}{\norm{\vec{f}_{1}}^2}\vec{f}_{1} + \frac{\vec{x} \dotp \vec{f}_{2}}{\norm{\vec{f}_{2}}^2}\vec{f}_{2} = \frac{4}{3}\vec{f}_{1} + \frac{-1}{3}\vec{f}_{2} = \frac{1}{3}\begin{bmatrix}
5 \\ 4 \\ -1 \\ 3
\end{bmatrix}
\end{equation*}
Thus, $\vec{p}$ is the vector in $W$ closest to $\vec{x}$, and $\vec{x} - \vec{p} = \frac{1}{3}\begin{bmatrix}
4 \\ -7 \\ 1 \\ 3
\end{bmatrix}$ is orthogonal to every vector in $W$. (This can be verified by checking that it is orthogonal to the generators $\vec{x}_{1}$ and $\vec{x}_{2}$ of $W$.) The required decomposition of $\vec{x}$ is thus
\begin{equation*}
\vec{x} = \vec{p} + (\vec{x} - \vec{p}) = \frac{1}{3}\begin{bmatrix}
5 \\ 4 \\ -1 \\ 3
\end{bmatrix} + \frac{1}{3}\begin{bmatrix}
4 \\ -7 \\ 1 \\ 3
\end{bmatrix}
\end{equation*}
\end{explanation}
\end{example}

\begin{example}\label{023934}
Find the point in the plane with equation $2x + y - z = 0$ that is closest to the point $(2, -1, -3)$.

\begin{explanation}
  We write the ordered triples of $\RR^3$ as rows. The plane is the subspace $W$ whose points $(x, y, z)$ satisfy $z = 2x + y$. Hence
\begin{equation*}
U = \{(s, t, 2s + t) \mid s,t \mbox{ in }\RR\} = \mbox{span}\left((0, 1, 1),(1, 0, 2)\}\right)
\end{equation*}
The Gram-Schmidt process produces an orthogonal basis $\{\vec{f}_{1}, \vec{f}_{2}\}$ of $W$ where $\vec{f}_{1} = (0, 1, 1)$ and $\vec{f}_{2} = (1, -1, 1)$. Hence, the vector in $W$ closest to $\vec{x} = (2, -1, -3)$ is
\begin{equation*}
\mbox{proj}_W(\vec{x}) =\frac{\vec{x} \dotp \vec{f}_{1}}{\norm{\vec{f}_{1}}^2}\vec{f}_{1} + \frac{\vec{x} \dotp \vec{f}_{2}}{\norm{\vec{f}_{2}}^2}\vec{f}_{2} = -2\vec{f}_{1} + 0\vec{f}_{2} = (0, -2, -2)
\end{equation*}
Thus, the point in $W$ closest to $(2, -1, -3)$ is $(0, -2, -2)$.
\end{explanation}
\end{example}

\section*{Practice Problems}

\begin{problem}
In each case, use the Gram-Schmidt algorithm to convert the given basis $B$ of $V$ into an orthogonal basis.  

\begin{problem}\label{GS1}
$V = \RR^2$, $B = \{(1, -1), (2, 1)\}$
$\{(\answer{2},\answer{1}),(\answer{-\frac{3}{5}},\answer{\frac{6}{5}})\}$
\end{problem}
\begin{problem}
$V = \RR^2$, $B = \{(2, 1), (1, 2)\}$
\end{problem}
\begin{problem}
$V = \RR^3$, $B = \{(1, -1, 1), (1, 0, 1), (1, 1, 2)\}$

$\{(\answer{0},\answer{1},\answer{1}),(\answer{1},\answer{1},\answer{1}),(\answer{1},\answer{-2},\answer{2})\}$
\end{problem}
\begin{problem}
$V = \RR^3$, $B = \{(0, 1, 1), (1, 1, 1), (1, -2, 2)\}$
\end{problem}
\end{problem}

\begin{problem}
In each case, use the Gram-Schmidt algorithm to find an orthogonal basis of the subspace $W$, and find the vector in $W$ closest to $\vec{x}$.

\begin{problem}\label{ClosestVec1}
$W = \mbox{span}\left((1, 1, 1), (0, 1, 1)\}\right)$, $\vec{x} = (-1, 2, 1)$
%ANSWER $\{(1, -1, 0), \frac{1}{2}(-1, -1, 2)\}$; $\mbox{proj}_W(\vec{x}) = (1, 0, -1)$
\end{problem}

\begin{problem}\label{ClosestVec2}
$W = \mbox{span}\left((1, -1, 0), (-1, 0, 1)\}\right)$, $\vec{x} = (2, 1, 0)$
\end{problem}

\begin{problem}\label{ClosestVec3}
$W = \mbox{span}\left((1, 0, 1, 0), (1, 1, 1, 0), (1, 1, 0, 0)\}\right)$, $\vec{x} = (2, 0, -1, 3)$
%ANSWER $\{(1, -1, 0, 1), (1, 1, 0, 0), \frac{1}{3}(-1, 1, 0, 2)\}$; $\mbox{proj}_W(\vec{x}) = (2, 0, 0, 1)$
\end{problem}

\begin{problem}\label{ClosestVec4}
$W = \mbox{span}\left((1, -1, 0, 1), (1, 1, 0, 0), (1, 1, 0, 1)\}\right)$, $\vec{x} = (2, 0, 3, 1)$
\end{problem}

\end{problem}

\begin{problem}
Let $A$ be an $n \times n$ matrix of rank $r$. Show that there is an invertible $n \times n$ matrix $M$ such that $MA$ is a row-echelon matrix with the property that the first $r$ rows are orthogonal. [\textit{Hint}: Let $R$ be the row-echelon form of $A$, and use the Gram-Schmidt process on the nonzero rows of $R$ from the bottom up. Use Lemma~\ref{004537}.]
\end{problem}

\begin{problem}
Let $A$ be an $(n - 1) \times n$ matrix with rows $\vec{x}_{1}, \vec{x}_{2}, \dots, \vec{x}_{n-1}$ and let $A_{i}$ denote the \\ $(n - 1) \times (n - 1)$ matrix obtained from $A$ by deleting column $i$. Define the vector $\vec{y}$ in $\RR^n$ by \begin{equation*}
\vec{y} = \begin{bmatrix}
\det A_{1} \\ -\det A_{2} \\ \det A_{3} \\ \cdots \\ (-1)^{n+1} \det A_{n}
\end{bmatrix}
\end{equation*} Show that:


\begin{enumerate}[label={\alph*.}]
\item $\vec{x}_{i} \dotp \vec{y} = 0$ for all $i = 1, 2, \dots , n - 1$. 
\begin{hint}
Write $B_{i} = \begin{bmatrix}
x_{i} \\ A
\end{bmatrix}$
and show that $\det B_{i} = 0$.
\end{hint}

\item $\vec{y} \neq \vec{0}$ if and only if $\{\vec{x}_{1}, \vec{x}_{2}, \dots , \vec{x}_{n-1}\}$ is linearly independent.
\begin{hint}
If some $\det A_{i} \neq 0$, the rows of $A_{i}$ are linearly independent. Conversely, if the $\vec{x}_{i}$ are independent, consider $A = UR$ where $R$ is in reduced row-echelon form
\end{hint}

\item If $\{\vec{x}_{1}, \vec{x}_{2}, \dots , \vec{x}_{n-1}\}$ is linearly independent, use Theorem~\ref{023885}(3) to show that all solutions to the system of $n - 1$ homogeneous equations
\begin{equation*}
A\vec{x}^T = \vec{0}
\end{equation*}
are given by $t\vec{y}$, $t$ a parameter.

\end{enumerate}
\end{problem}

\section*{Text Source} This section was adapted from the first part of Section 8.1 of Keith Nicholson's \href{https://open.umn.edu/opentextbooks/textbooks/linear-algebra-with-applications}{\it Linear Algebra with Applications}. (CC-BY-NC-SA)

W. Keith Nicholson, {\it Linear Algebra with Applications}, Lyryx 2018, Open Edition, p. 415 

%\section*{Example Source}
%Examples \ref{ex:polyindset} and \ref{ex:CAbasis} were adapted from Examples 6.3.1 and 6.3.10 of Keith Nicholson's \href{https://open.umn.edu/opentextbooks/textbooks/linear-algebra-with-applications}{\it Linear Algebra with Applications}. (CC-BY-NC-SA)

%W. Keith Nicholson, {\it Linear Algebra with Applications}, Lyryx 2018, Open Edition, p. 346, 350

%\section*{Exercise Source}
%Practice Problems \ref{prob:linindabstractvsp1}, \ref{prob:linindabstractvsp2} and \ref{prob:linindabstractvsp3} are Exercises 6.3(a)(b)(c) from Keith Nicholson's \href{https://open.umn.edu/opentextbooks/textbooks/linear-algebra-with-applications}{\it Linear Algebra with Applications}. (CC-BY-NC-SA)

%W. Keith Nicholson, {\it Linear Algebra with Applications}, Lyryx 2018, Open Edition, p. 351



\end{document}